"""
DKR Agent - åŸºäº LangGraph çš„è‡ªä¸» Agent å®ç°
"""
from typing import Dict, Any, Optional
from datetime import datetime
from loguru import logger

from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage

from app.core.library_manager import LibraryManager
from app.core.llm_client import DeepSeekLLMClient
from app.config import get_settings


# å…¨å±€å®ä¾‹ï¼ˆç”¨äºå·¥å…·å‡½æ•°è®¿é—®ï¼‰
_library_manager: Optional[LibraryManager] = None
_llm_client: Optional[DeepSeekLLMClient] = None


def _init_globals():
    """åˆå§‹åŒ–å…¨å±€å®ä¾‹"""
    global _library_manager, _llm_client
    if _library_manager is None:
        _library_manager = LibraryManager()
    if _llm_client is None:
        _llm_client = DeepSeekLLMClient()


# LangGraph å·¥å…·å®šä¹‰
@tool
def get_library_catalog(query: str = "") -> str:
    """
    è·å–æ–‡æ¡£åº“çš„å®Œæ•´ç›®å½•ï¼ˆæ‰€æœ‰åˆ†ç±» + æ‰€æœ‰æ–‡æ¡£ï¼‰ã€‚

    è¿™ä¸ªå·¥å…·ä¼šä¸€æ¬¡æ€§è¿”å›ï¼š
    1. æ‰€æœ‰åˆ†ç±»åˆ—è¡¨
    2. æ¯ä¸ªåˆ†ç±»ä¸‹çš„æ‰€æœ‰æ–‡æ¡£ï¼ˆæ–‡ä»¶åã€é¡µæ•°ã€æ–‡æ¡£æ‘˜è¦ï¼‰

    é€‚ç”¨äºï¼šå¿«é€Ÿæµè§ˆæ•´ä¸ªæ–‡æ¡£åº“ï¼Œåˆ¤æ–­è¦æŸ¥çœ‹å“ªäº›æ–‡æ¡£ã€‚

    Args:
        query: ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—è®°å½•ï¼‰

    Returns:
        å®Œæ•´çš„æ–‡æ¡£åº“ç›®å½•ï¼ˆæ‰€æœ‰åˆ†ç±»å’Œæ–‡æ¡£ï¼‰
    """
    _init_globals()
    logger.info(f"[Tool] get_library_catalog: {query}")

    # è·å–æ‰€æœ‰åˆ†ç±»
    categories = _library_manager.list_categories()

    if not categories:
        return "æ–‡æ¡£åº“ä¸ºç©ºï¼Œæ²¡æœ‰ä»»ä½•åˆ†ç±»å’Œæ–‡æ¡£"

    result = "ã€æ–‡æ¡£åº“å®Œæ•´ç›®å½•ã€‘\n\n"
    result += f"å…± {len(categories)} ä¸ªåˆ†ç±»\n\n"
    result += "=" * 80 + "\n\n"

    total_docs = 0

    for category in categories:
        category_name = category.get('name', 'æœªå‘½ååˆ†ç±»')
        doc_count = category.get('doc_count', 0)
        total_docs += doc_count

        result += f"ğŸ“ åˆ†ç±»ï¼š{category_name}ï¼ˆ{doc_count} ä»½æ–‡æ¡£ï¼‰\n"
        result += f"{'-' * 80}\n"

        # è·å–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰æ–‡æ¡£
        documents = _library_manager.list_documents(category=category_name)

        if documents:
            for i, doc in enumerate(documents, 1):
                metadata = doc.get('metadata', {})

                # æ–‡ä»¶å
                filename = metadata.get('filename', doc.get('title', doc['doc_id']))

                # é¡µæ•°
                page_count = metadata.get('page_count', 'æœªçŸ¥')

                # æ–‡æ¡£æ‘˜è¦
                doc_summary = metadata.get('doc_summary', 'æ— æ‘˜è¦')

                result += f"  {i}. {filename}\n"
                result += f"     - æ–‡æ¡£ ID: {doc['doc_id']}\n"
                result += f"     - é¡µæ•°: {page_count} é¡µ\n"
                result += f"     - æ‘˜è¦: {doc_summary}\n"
                result += f"\n"
        else:
            result += f"  ï¼ˆè¯¥åˆ†ç±»ä¸‹æš‚æ— æ–‡æ¡£ï¼‰\n\n"

        result += "\n"

    result += "=" * 80 + "\n"
    result += f"ã€ç»Ÿè®¡ã€‘å…± {len(categories)} ä¸ªåˆ†ç±»ï¼Œ{total_docs} ä»½æ–‡æ¡£\n\n"
    result += "ã€ä¸‹ä¸€æ­¥ã€‘è¯·é€‰æ‹©æ‚¨æƒ³æŸ¥çœ‹çš„æ–‡æ¡£ï¼ˆå¯ä»¥æ˜¯ 1 ä¸ªæˆ–å¤šä¸ªï¼‰ï¼Œæˆ‘ä¼šè¿”å›è¿™äº›æ–‡æ¡£çš„ç›®å½•ï¼ˆæ‰€æœ‰é¡µé¢çš„æ‘˜è¦ï¼‰ã€‚\n"

    return result


@tool
def get_documents_table_of_contents(doc_ids: list, query: str = "") -> str:
    """
    è·å–ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡æ¡£çš„ç›®å½•ï¼ˆæ‰€æœ‰é¡µé¢çš„ page_summaryï¼‰ã€‚

    è¿™ä¸ªå·¥å…·ä¼šè¿”å›æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰é¡µé¢æ‘˜è¦ï¼Œåƒç¿»é˜…ç›®å½•ä¸€æ ·å¿«é€Ÿäº†è§£æ–‡æ¡£ç»“æ„ã€‚

    å·¥ä½œæµç¨‹ï¼š
    1. è¯»å–æŒ‡å®šæ–‡æ¡£çš„ summaries.json
    2. æå–æ‰€æœ‰é¡µé¢çš„ page_summary
    3. è¿”å›ç®€æ´çš„ç›®å½•æ ¼å¼

    é€‚ç”¨äºï¼šå¿«é€Ÿæµè§ˆæ–‡æ¡£å†…å®¹ï¼Œå®šä½æ„Ÿå…´è¶£çš„é¡µé¢ã€‚

    Args:
        doc_ids: æ–‡æ¡£ ID åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯ 1 ä¸ªæˆ–å¤šä¸ªï¼Œä¾‹å¦‚ ["doc_xxx", "doc_yyy"]ï¼‰
        query: ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—è®°å½•ï¼‰

    Returns:
        æ–‡æ¡£ç›®å½•ï¼ˆæ‰€æœ‰é¡µé¢çš„ page_summaryï¼‰
    """
    _init_globals()
    logger.info(f"[Tool] get_documents_table_of_contents: doc_ids={doc_ids}, query={query}")

    try:
        import sys
        import json
        from pathlib import Path

        # Add project root to path
        project_root = Path(__file__).parent.parent.parent.parent
        sys.path.insert(0, str(project_root))

        from app.core.library_manager import LibraryManager

        library_manager = LibraryManager()

        result = "ã€æ–‡æ¡£ç›®å½•ã€‘\n\n"

        for doc_id in doc_ids:
            # è·å–æ–‡æ¡£ä¿¡æ¯
            doc_info = library_manager.get_document(doc_id)
            if not doc_info:
                result += f"âš ï¸ é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} ä¸å­˜åœ¨\n\n"
                continue

            metadata = doc_info.get("metadata", {})
            filename = metadata.get("filename", doc_id)
            page_count = metadata.get("page_count", 0)
            summary_path = metadata.get("summary_path")

            if not summary_path:
                result += f"âš ï¸ é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} ç¼ºå°‘ Summary æ–‡ä»¶\n\n"
                continue

            # è¯»å– Summary JSON
            with open(summary_path, 'r', encoding='utf-8') as f:
                summary_data = json.load(f)

            # æ£€æŸ¥æ ¼å¼
            if not isinstance(summary_data, list):
                result += f"âš ï¸ é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} çš„ Summary æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®\n\n"
                continue

            result += f"ğŸ“„ æ–‡æ¡£ï¼š{filename}\n"
            result += f"   æ–‡æ¡£ ID: {doc_id}\n"
            result += f"   æ€»é¡µæ•°: {page_count} é¡µ\n"
            result += f"{'-' * 80}\n"

            # æå–æ‰€æœ‰é¡µé¢çš„ page_summary
            for page_data in summary_data:
                page_num = page_data.get("page_num", "?")
                page_summary = page_data.get("page_summary", "æ— æ‘˜è¦")

                result += f"  ç¬¬ {page_num} é¡µï¼š{page_summary}\n"

            result += f"\n"

        result += "=" * 80 + "\n"
        result += "ã€ä¸‹ä¸€æ­¥ã€‘è¯·é€‰æ‹©æ‚¨æ„Ÿå…´è¶£çš„é¡µé¢ï¼Œæˆ‘ä¼šè¿”å›è¿™äº›é¡µé¢çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬å®ä½“ã€æ•°æ®ã€è¡¨æ ¼ç­‰ï¼‰ã€‚\n"

        return result

    except Exception as e:
        logger.error(f"get_documents_table_of_contents error: {e}", exc_info=True)
        return f"è·å–æ–‡æ¡£ç›®å½•å‡ºé”™ï¼š{str(e)}"


@tool
def get_pages_full_summary(doc_id: str, page_nums: list) -> str:
    """
    è·å–æŒ‡å®šé¡µé¢çš„å®Œæ•´ Summary ä¿¡æ¯ã€‚

    è¿™ä¸ªå·¥å…·ä¼šè¿”å›æŒ‡å®šé¡µé¢çš„è¯¦ç»† Summaryï¼ŒåŒ…æ‹¬ï¼š
    - page_summaryï¼ˆé¡µé¢æ‘˜è¦ï¼‰
    - entitiesï¼ˆå…³é”®å®ä½“ï¼‰
    - key_dataï¼ˆå…³é”®æ•°æ®ï¼‰
    - table_infoï¼ˆè¡¨æ ¼ä¿¡æ¯ï¼‰
    - chart_infoï¼ˆå›¾è¡¨ä¿¡æ¯ï¼‰
    - image_infoï¼ˆå›¾åƒä¿¡æ¯ï¼‰

    é€‚ç”¨äºï¼šåœ¨ç›®å½•ä¸­å®šä½åˆ°æ„Ÿå…´è¶£çš„é¡µé¢åï¼ŒæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ã€‚

    Args:
        doc_id: æ–‡æ¡£ ID
        page_nums: é¡µç åˆ—è¡¨ï¼ˆä¾‹å¦‚ [1, 3, 5, 61]ï¼‰

    Returns:
        æŒ‡å®šé¡µé¢çš„å®Œæ•´ Summary ä¿¡æ¯
    """
    _init_globals()
    logger.info(f"[Tool] get_pages_full_summary: doc_id={doc_id}, page_nums={page_nums}")

    try:
        import sys
        import json
        from pathlib import Path

        # Add project root to path
        project_root = Path(__file__).parent.parent.parent.parent
        sys.path.insert(0, str(project_root))

        from app.core.library_manager import LibraryManager

        library_manager = LibraryManager()

        # è·å–æ–‡æ¡£ä¿¡æ¯
        doc_info = library_manager.get_document(doc_id)
        if not doc_info:
            return f"é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} ä¸å­˜åœ¨"

        metadata = doc_info.get("metadata", {})
        summary_path = metadata.get("summary_path")

        if not summary_path:
            return f"é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} ç¼ºå°‘ Summary æ–‡ä»¶"

        # è¯»å– Summary JSON
        with open(summary_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)

        # æ£€æŸ¥æ ¼å¼
        if not isinstance(summary_data, list):
            return f"é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} çš„ Summary æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®"

        result = f"ã€é¡µé¢è¯¦ç»†ä¿¡æ¯ã€‘\n"
        result += f"æ–‡æ¡£: {doc_id}\n"
        result += f"æŸ¥çœ‹ {len(page_nums)} ä¸ªé¡µé¢\n\n"
        result += "=" * 80 + "\n\n"

        for page_num in page_nums:
            # æŸ¥æ‰¾å¯¹åº”çš„é¡µé¢æ•°æ®ï¼ˆpage_num ä» 1 å¼€å§‹ï¼Œç´¢å¼•ä» 0 å¼€å§‹ï¼‰
            page_data = None
            for data in summary_data:
                if data.get("page_num") == page_num:
                    page_data = data
                    break

            if not page_data:
                result += f"âš ï¸ ç¬¬ {page_num} é¡µï¼šæœªæ‰¾åˆ° Summary æ•°æ®\n\n"
                continue

            result += f"ã€ç¬¬ {page_num} é¡µã€‘\n"
            result += f"{'-' * 80}\n"

            # é¡µé¢ç±»å‹
            page_type = page_data.get("page_type", "æœªçŸ¥")
            result += f"é¡µé¢ç±»å‹ï¼š{page_type}\n\n"

            # é¡µé¢æ‘˜è¦
            page_summary = page_data.get("page_summary", "æ— æ‘˜è¦")
            result += f"é¡µé¢æ‘˜è¦ï¼š\n{page_summary}\n\n"

            # å…³é”®å®ä½“
            entities = page_data.get("entities", [])
            if entities:
                result += f"å…³é”®å®ä½“ï¼ˆ{len(entities)} ä¸ªï¼‰ï¼š\n"
                # åªæ˜¾ç¤ºå‰ 20 ä¸ªå®ä½“
                display_entities = entities[:20]
                result += f"{', '.join(display_entities)}\n"
                if len(entities) > 20:
                    result += f"...ï¼ˆè¿˜æœ‰ {len(entities) - 20} ä¸ªå®ä½“ï¼‰\n"
                result += f"\n"

            # å…³é”®æ•°æ®
            key_data = page_data.get("key_data", [])
            if key_data:
                result += f"å…³é”®æ•°æ®ï¼š\n"
                for data in key_data:
                    key = data.get("key", "")
                    value = data.get("value", "")
                    result += f"  - {key}: {value}\n"
                result += f"\n"

            # è¡¨æ ¼ä¿¡æ¯
            table_info = page_data.get("table_info")
            if table_info:
                result += f"è¡¨æ ¼ä¿¡æ¯ï¼š\n"
                title = table_info.get("title", "æ— æ ‡é¢˜")
                result += f"  æ ‡é¢˜ï¼š{title}\n"

                columns = table_info.get("columns", [])
                if columns:
                    result += f"  åˆ—åï¼š{', '.join(columns)}\n"

                rows_data = table_info.get("rows_data", "")
                if rows_data:
                    # é™åˆ¶é•¿åº¦
                    display_data = rows_data[:500]
                    result += f"  æ•°æ®ï¼š{display_data}\n"
                    if len(rows_data) > 500:
                        result += f"  ...ï¼ˆæ•°æ®è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼‰\n"
                result += f"\n"

            # å›¾è¡¨ä¿¡æ¯
            chart_info = page_data.get("chart_info")
            if chart_info:
                result += f"å›¾è¡¨ä¿¡æ¯ï¼š\n"
                chart_type = chart_info.get("type", "æœªçŸ¥")
                description = chart_info.get("description", "æ— æè¿°")
                result += f"  ç±»å‹ï¼š{chart_type}\n"
                result += f"  æè¿°ï¼š{description}\n"
                result += f"\n"

            # å›¾åƒä¿¡æ¯
            image_info = page_data.get("image_info")
            if image_info:
                result += f"å›¾åƒä¿¡æ¯ï¼š\n"
                description = image_info.get("description", "æ— æè¿°")
                result += f"  æè¿°ï¼š{description}\n"

                key_elements = image_info.get("key_elements", [])
                if key_elements:
                    result += f"  å…³é”®å…ƒç´ ï¼š{', '.join(key_elements)}\n"
                result += f"\n"

            result += f"{'-' * 80}\n\n"

        result += "=" * 80 + "\n"
        result += "ã€ä¸‹ä¸€æ­¥ã€‘å¦‚æœ Summary ä¿¡æ¯è¶³å¤Ÿï¼Œè¯·ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚å¦‚æœéœ€è¦æŸ¥çœ‹åŸæ–‡ï¼Œè¯·ä½¿ç”¨ search_in_document è¿›è¡Œå…¨é‡ OCRã€‚\n"

        return result

    except Exception as e:
        logger.error(f"get_pages_full_summary error: {e}", exc_info=True)
        return f"è·å–é¡µé¢è¯¦ç»†ä¿¡æ¯å‡ºé”™ï¼š{str(e)}"


@tool
def search_in_document(doc_id: str, page_nums: list, query: str = "") -> str:
    """
    ã€å…¨é‡ OCR å·¥å…·ã€‘å¯¹æŒ‡å®šé¡µé¢è¿›è¡Œå…¨é‡ OCRï¼ˆä½¿ç”¨ DeepSeek OCR APIï¼‰ã€‚

    âš ï¸âš ï¸âš ï¸ ä¸¥é‡è­¦å‘Šï¼šè¿™æ˜¯æˆæœ¬æé«˜ã€é€Ÿåº¦ææ…¢çš„æ“ä½œï¼

    âš ï¸ ä½¿ç”¨å‰ææ¡ä»¶ï¼ˆå¿…é¡»æ»¡è¶³ï¼‰ï¼š
    1. å¿…é¡»å…ˆè°ƒç”¨ get_documents_table_of_contents æŸ¥çœ‹ç›®å½•
    2. å¿…é¡»å…ˆè°ƒç”¨ get_pages_full_summary æŸ¥çœ‹è¯¦ç»† Summary
    3. å¿…é¡»ç¡®è®¤ Summary å®Œå…¨ä¸è¶³ä»¥å›ç­”é—®é¢˜
    4. å¿…é¡»è¯´æ˜ä¸ºä»€ä¹ˆ Summary ä¸è¶³ï¼ˆè®°å½•å†³ç­–ç†ç”±ï¼‰

    âš ï¸ å¦‚æœæœªæ»¡è¶³ä¸Šè¿°æ¡ä»¶ï¼Œç¦æ­¢è°ƒç”¨æ­¤å·¥å…·ï¼

    é€‚ç”¨åœºæ™¯ï¼ˆä»…é™ä»¥ä¸‹æƒ…å†µï¼‰ï¼š
    - Summary ä¿¡æ¯ä¸¥é‡ä¸è¶³ï¼Œæ— æ³•å›ç­”é—®é¢˜
    - éœ€è¦æŸ¥çœ‹å›¾è¡¨ã€è¡¨æ ¼çš„è¯¦ç»†å†…å®¹
    - éœ€è¦ç²¾ç¡®çš„æ•°å­—ã€å…¬å¼ã€ä»£ç ç­‰

    å·¥ä½œæµç¨‹ï¼š
    1. å¯¹æŒ‡å®šçš„é¡µé¢è¿›è¡Œå…¨é‡ OCRï¼ˆè€—æ—¶ 3-5 ç§’/é¡µï¼‰
    2. è¿”å› OCR ç»“æœ

    Args:
        doc_id: æ–‡æ¡£ ID
        page_nums: è¦ OCR çš„é¡µç åˆ—è¡¨ï¼ˆä¾‹å¦‚ [1, 3, 5]ï¼Œå»ºè®®ä¸è¶…è¿‡ 5 é¡µï¼‰
        query: ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—è®°å½•ï¼‰

    Returns:
        å…¨é‡ OCR ç»“æœ
    """
    _init_globals()

    # å®‰å…¨æ£€æŸ¥
    if not page_nums or len(page_nums) == 0:
        return "é”™è¯¯ï¼šå¿…é¡»æŒ‡å®šè¦ OCR çš„é¡µç åˆ—è¡¨ï¼ˆpage_nums å‚æ•°ï¼‰"

    if len(page_nums) > 5:
        logger.warning(f"[Tool] âš ï¸ è¯·æ±‚ OCR {len(page_nums)} é¡µï¼Œè¶…è¿‡å»ºè®®çš„ 5 é¡µé™åˆ¶")
        return (
            f"âš ï¸ è­¦å‘Šï¼šæ‚¨è¯·æ±‚ OCR {len(page_nums)} é¡µï¼Œè¶…è¿‡å»ºè®®çš„ 5 é¡µé™åˆ¶ã€‚\n"
            f"å»ºè®®ï¼šè¯·ä»ä¸­é€‰æ‹© 3-5 é¡µæœ€ç›¸å…³çš„é¡µé¢ã€‚\n"
            f"åŸå› ï¼šå…¨é‡ OCR æˆæœ¬é«˜ã€é€Ÿåº¦æ…¢ï¼Œåº”ç²¾å‡†é€‰æ‹©é¡µé¢ã€‚"
        )

    logger.info(f"[Tool] search_in_document: doc_id={doc_id}, page_nums={page_nums}, query={query}")
    logger.info(f"[Tool] å°†è°ƒç”¨ DeepSeek OCR API è¿›è¡Œå®æ—¶æ–‡æ¡£ç†è§£")

    try:
        import sys
        from pathlib import Path

        # Add project root to path (to import visual_memvid)
        project_root = Path(__file__).parent.parent.parent.parent
        sys.path.insert(0, str(project_root))

        from visual_memvid.visual_retriever import VisualMemvidRetriever
        from visual_memvid.ocr_client import DeepSeekOCRClient
        from app.config import get_settings
        from app.core.library_manager import LibraryManager

        settings = get_settings()
        library_manager = LibraryManager()

        # è·å–æ–‡æ¡£ä¿¡æ¯
        doc_info = library_manager.get_document(doc_id)
        if not doc_info:
            return f"é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} ä¸å­˜åœ¨"

        metadata = doc_info.get("metadata", {})
        video_path = metadata.get("video_path")
        index_path = metadata.get("index_path")

        if not video_path or not index_path:
            return f"é”™è¯¯ï¼šæ–‡æ¡£ {doc_id} ç¼ºå°‘è§†é¢‘æˆ–ç´¢å¼•æ–‡ä»¶"

        # åˆå§‹åŒ– OCR å®¢æˆ·ç«¯å’Œ visual retriever
        ocr_client = DeepSeekOCRClient(endpoint=settings.ocr_api_url)
        logger.info(f"[Tool] æ­£åœ¨è°ƒç”¨ DeepSeek OCR API: {settings.ocr_api_url}")

        visual_retriever = VisualMemvidRetriever(
            video_path=video_path,
            index_path=index_path,
            ocr_client=ocr_client,
            enable_cache=True
        )

        # ç²¾å‡† OCRï¼šåªå¤„ç†æŒ‡å®šçš„é¡µé¢
        logger.info(f"[Tool] ç²¾å‡† OCR æ¨¡å¼ï¼šå¤„ç†æŒ‡å®šçš„ {len(page_nums)} é¡µ: {page_nums}")
        results = []
        for page_num in page_nums:
            try:
                # æå–å¸§å¹¶ OCRï¼ˆé¡µç ä» 1 å¼€å§‹ï¼Œframe_num ä» 0 å¼€å§‹ï¼‰
                frame_num = page_num - 1
                frame = visual_retriever._extract_frame(frame_num)
                if frame is not None:
                    ocr_result = ocr_client.ocr_image(frame)

                    # æ£€æŸ¥ OCR ç»“æœæ˜¯å¦ä¸º None
                    if ocr_result is None:
                        logger.warning(f"[Tool] âš ï¸ ç¬¬ {page_num} é¡µ OCR è¿”å› None")
                        continue

                    if ocr_result.get("success"):
                        content = ocr_result.get("text", "")
                        results.append({
                            "page_num": page_num,
                            "frame_num": frame_num,
                            "content": content,
                            "page_type": "OCR"
                        })
                        logger.info(f"[Tool] âœ… ç¬¬ {page_num} é¡µ OCR æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                    else:
                        error_msg = ocr_result.get("error", "æœªçŸ¥é”™è¯¯")
                        logger.warning(f"[Tool] âš ï¸ ç¬¬ {page_num} é¡µ OCR å¤±è´¥: {error_msg}")
                else:
                    logger.warning(f"[Tool] âš ï¸ ç¬¬ {page_num} é¡µå¸§æå–å¤±è´¥")
            except Exception as e:
                logger.error(f"[Tool] âŒ ç¬¬ {page_num} é¡µå¤„ç†å‡ºé”™: {e}", exc_info=True)

        if results:
            logger.info(f"[Tool] DeepSeek OCR æˆåŠŸå¤„ç† {len(results)} ä¸ªé¡µé¢")

            response = f"ã€å…¨é‡ OCR ç»“æœã€‘\n"
            response += f"æ–‡æ¡£: {doc_id}\n"
            response += f"å¤„ç†äº† {len(results)} ä¸ªé¡µé¢ï¼ˆæŒ‡å®šé¡µç : {page_nums}ï¼‰\n\n"
            response += "=" * 80 + "\n\n"

            for i, page_result in enumerate(results, 1):
                page_num = page_result.get('page_num', '?')
                content = page_result.get('content', '')

                response += f"ã€é¡µé¢ {i}/{len(results)}ã€‘ç¬¬ {page_num} é¡µ\n"
                response += f"{'-' * 80}\n"
                response += f"{content}\n"
                response += f"{'-' * 80}\n\n"

            return response
        else:
            return f"OCR å¤±è´¥ï¼šæœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•é¡µé¢"

    except Exception as e:
        logger.error(f"search_in_document error: {e}", exc_info=True)
        return f"æœç´¢å‡ºé”™ï¼š{str(e)}"


@tool
def evaluate_answer_confidence(query: str, answer: str) -> str:
    """
    è¯„ä¼°ç­”æ¡ˆçš„ç½®ä¿¡åº¦ã€‚
    é€‚ç”¨äºï¼šè·å¾—ç­”æ¡ˆåï¼Œåˆ¤æ–­ç­”æ¡ˆè´¨é‡æ˜¯å¦æ»¡è¶³è¦æ±‚ã€‚

    Args:
        query: åŸå§‹æŸ¥è¯¢é—®é¢˜
        answer: è·å¾—çš„ç­”æ¡ˆ

    Returns:
        ç½®ä¿¡åº¦è¯„åˆ†å’Œå»ºè®®ï¼ˆ0-1ä¹‹é—´ï¼Œ>0.9 è¡¨ç¤ºé«˜è´¨é‡ç­”æ¡ˆï¼‰
    """
    _init_globals()
    logger.info(f"[Tool] evaluate_answer_confidence")

    try:
        confidence = _llm_client.evaluate_confidence(query, answer, [])

        if confidence >= 0.9:
            return f"ç½®ä¿¡åº¦: {confidence:.2f} - ç­”æ¡ˆè´¨é‡å¾ˆé«˜ï¼Œå¯ä»¥è¿”å›ç»™ç”¨æˆ·"
        elif confidence >= 0.7:
            return f"ç½®ä¿¡åº¦: {confidence:.2f} - ç­”æ¡ˆè´¨é‡ä¸­ç­‰ï¼Œå»ºè®®ç»§ç»­æœç´¢æ›´å¤šä¿¡æ¯"
        else:
            return f"ç½®ä¿¡åº¦: {confidence:.2f} - ç­”æ¡ˆè´¨é‡è¾ƒä½ï¼Œéœ€è¦é‡æ–°æœç´¢"

    except Exception as e:
        logger.error(f"evaluate_answer_confidence error: {e}")
        return f"è¯„ä¼°å‡ºé”™ï¼š{str(e)}"


# æ—§å·¥å…·å·²åˆ é™¤ï¼šsearch_in_document_summaryï¼ˆè¢« get_documents_table_of_contents + get_pages_full_summary æ›¿ä»£ï¼‰
# æ—§å·¥å…·å·²åˆ é™¤ï¼šget_full_document_contentï¼ˆä¸å†éœ€è¦ï¼‰


class DKRAgent:
    """
    DKR Agent - åŸºäº LangGraph çš„è‡ªä¸»æ–‡æ¡£æ£€ç´¢ Agent

    ç‰¹æ€§ï¼š
    - è‡ªä¸»å†³ç­–å·¥å…·è°ƒç”¨é¡ºåº
    - å¾ªç¯è°ƒç”¨ç›´åˆ°æ‰¾åˆ°æ»¡æ„ç­”æ¡ˆ
    - æ”¯æŒçŠ¶æ€æŒä¹…åŒ–
    - ç±»ä¼¼ Claude Agentic Search çš„å·¥ä½œæ–¹å¼
    """

    def __init__(self):
        self.settings = get_settings()
        self.confidence_threshold = self.settings.agent_confidence_threshold

        # åˆå§‹åŒ–å…¨å±€å®ä¾‹
        _init_globals()

        # åˆ›å»º LangChain LLMï¼ˆæ ¹æ®é…ç½®é€‰æ‹© DeepSeek æˆ– Geminiï¼‰
        if self.settings.agent_llm_provider == "gemini":
            logger.info(f"ä½¿ç”¨ Gemini æ¨¡å‹: {self.settings.agent_llm_model}")
            self.llm = ChatOpenAI(
                base_url=self.settings.openrouter_base_url,
                api_key=self.settings.openrouter_api_key,
                model=self.settings.agent_llm_model,
                temperature=0.3
            )
        else:
            logger.info(f"ä½¿ç”¨ DeepSeek æ¨¡å‹: {self.settings.deepseek_model}")
            self.llm = ChatOpenAI(
                base_url=self.settings.deepseek_base_url,
                api_key=self.settings.deepseek_api_key,
                model=self.settings.deepseek_model,
                temperature=0.3
            )

        # å®šä¹‰å·¥å…·åˆ—è¡¨ï¼ˆæ–°ç‰ˆæœ¬ï¼š5ä¸ªå·¥å…·ï¼‰
        self.tools = [
            get_library_catalog,                # å·¥å…·1: è·å–æ–‡æ¡£åº“å®Œæ•´ç›®å½•
            get_documents_table_of_contents,    # å·¥å…·2: è·å–æ–‡æ¡£ç›®å½•ï¼ˆæ‰€æœ‰ page_summaryï¼‰
            get_pages_full_summary,             # å·¥å…·3: è·å–é¡µé¢è¯¦ç»†ä¿¡æ¯
            search_in_document,                 # å·¥å…·4: å…¨é‡ OCR
            evaluate_answer_confidence          # å·¥å…·5: è¯„ä¼°ç­”æ¡ˆç½®ä¿¡åº¦
        ]

        # åˆ›å»º Agentï¼ˆå¸¦çŠ¶æ€æŒä¹…åŒ–ï¼‰
        self.memory = MemorySaver()
        self.agent = create_react_agent(
            self.llm,
            self.tools,
            state_modifier=self._get_system_prompt(),
            checkpointer=self.memory
        )

        logger.info("DKRAgent initialized with LangGraph")

    def _get_system_prompt(self) -> str:
        """è·å– Agent ç³»ç»Ÿæç¤ºè¯ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰"""
        from pathlib import Path

        # è¯»å– Prompt æ–‡ä»¶ï¼ˆbackend/prompts/agent_system_prompt.txtï¼‰
        # dkr_agent.py åœ¨ backend/app/agent/ï¼Œæ‰€ä»¥éœ€è¦ parent.parent.parent
        prompt_file = Path(__file__).parent.parent.parent / "prompts" / "agent_system_prompt.txt"

        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompt_template = f.read()

            # æ›¿æ¢å ä½ç¬¦
            prompt = prompt_template.format(
                confidence_threshold=self.confidence_threshold
            )

            return prompt
        except Exception as e:
            logger.error(f"è¯»å– Agent Prompt æ–‡ä»¶å¤±è´¥: {e}")
            # é™çº§åˆ°é»˜è®¤ Prompt
            return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£æ£€ç´¢åŠ©æ‰‹ã€‚ç½®ä¿¡åº¦é˜ˆå€¼ï¼š{self.confidence_threshold}"""
    
    async def ask(
        self,
        query: str,
        thread_id: str = "default"
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆLangGraph Agent è‡ªä¸»å¾ªç¯ï¼‰

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            thread_id: ä¼šè¯çº¿ç¨‹ IDï¼ˆç”¨äºçŠ¶æ€æŒä¹…åŒ–ï¼‰

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        start_time = datetime.now()
        logger.info(f"Agent å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")

        try:
            # é…ç½®ä¼šè¯çŠ¶æ€å’Œé€’å½’é™åˆ¶
            config = {
                "configurable": {"thread_id": thread_id},
                "recursion_limit": 50  # å¢åŠ é€’å½’é™åˆ¶åˆ° 50ï¼ˆé»˜è®¤ 25ï¼‰
            }

            # è°ƒç”¨ LangGraph Agentï¼ˆè‡ªä¸»å¾ªç¯è°ƒç”¨å·¥å…·ï¼‰
            logger.info("=" * 80)
            logger.info(f"ã€Agent å¼€å§‹æ‰§è¡Œã€‘")
            logger.info(f"æŸ¥è¯¢: {query}")
            logger.info(f"Thread ID: {thread_id}")
            logger.info("=" * 80)

            result = await self.agent.ainvoke(
                {"messages": [HumanMessage(content=query)]},
                config=config
            )

            # æå–æœ€ç»ˆç­”æ¡ˆ
            messages = result.get("messages", [])
            if not messages:
                return self._create_response(
                    success=False,
                    error="Agent æœªè¿”å›ä»»ä½•æ¶ˆæ¯",
                    processing_time=(datetime.now() - start_time).total_seconds()
                )

            # æ—¥å¿—ï¼šè®°å½• Agent çš„å®Œæ•´æ‰§è¡Œè¿‡ç¨‹
            logger.info("\n" + "=" * 80)
            logger.info(f"ã€Agent æ‰§è¡Œè¿‡ç¨‹ã€‘å…± {len(messages)} æ¡æ¶ˆæ¯")
            logger.info("=" * 80)

            for i, msg in enumerate(messages, 1):
                msg_type = getattr(msg, 'type', 'unknown')

                if msg_type == 'human':
                    logger.info(f"\n[{i}] ğŸ‘¤ ç”¨æˆ·æ¶ˆæ¯:")
                    logger.info(f"    {msg.content[:200]}")

                elif msg_type == 'ai':
                    logger.info(f"\n[{i}] ğŸ¤– Agent æ€è€ƒ:")
                    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                    if hasattr(msg, 'tool_calls') and msg.tool_calls:
                        for tool_call in msg.tool_calls:
                            tool_name = tool_call.get('name', 'unknown')
                            tool_args = tool_call.get('args', {})
                            logger.info(f"    ğŸ“ è°ƒç”¨å·¥å…·: {tool_name}")
                            logger.info(f"    ğŸ“ å‚æ•°: {tool_args}")
                    else:
                        # Agent çš„æœ€ç»ˆå›ç­”
                        content = msg.content[:500] if hasattr(msg, 'content') else str(msg)[:500]
                        logger.info(f"    ğŸ’¬ å›ç­”: {content}")

                elif msg_type == 'tool':
                    logger.info(f"\n[{i}] ğŸ”§ å·¥å…·è¿”å›:")
                    tool_name = getattr(msg, 'name', 'unknown')
                    content = msg.content[:300] if hasattr(msg, 'content') else str(msg)[:300]
                    logger.info(f"    å·¥å…·: {tool_name}")
                    logger.info(f"    ç»“æœ: {content}...")

                else:
                    logger.info(f"\n[{i}] â“ æœªçŸ¥æ¶ˆæ¯ç±»å‹: {msg_type}")

            # æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯ Agent çš„æœ€ç»ˆå›å¤
            final_message = messages[-1]
            answer = final_message.content if hasattr(final_message, 'content') else str(final_message)

            processing_time = (datetime.now() - start_time).total_seconds()

            logger.info("\n" + "=" * 80)
            logger.info(f"ã€Agent å®Œæˆã€‘")
            logger.info(f"æ‰§è¡Œæ­¥éª¤: {len(messages)} æ¡æ¶ˆæ¯")
            logger.info(f"è€—æ—¶: {processing_time:.2f}s")
            logger.info(f"æœ€ç»ˆç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
            logger.info("=" * 80)
            logger.info("\nğŸ“ ã€æœ€ç»ˆç­”æ¡ˆã€‘")
            logger.info("=" * 80)
            logger.info(answer)
            logger.info("=" * 80)

            return self._create_response(
                success=True,
                answer=answer,
                processing_time=processing_time
            )

        except Exception as e:
            logger.error(f"Agent å¤„ç†å¤±è´¥: {e}", exc_info=True)
            processing_time = (datetime.now() - start_time).total_seconds()
            return self._create_response(
                success=False,
                error=str(e),
                processing_time=processing_time
            )
    
    def _create_response(
        self,
        success: bool,
        answer: Optional[str] = None,
        processing_time: float = 0.0,
        error: Optional[str] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºå“åº”ï¼ˆåªè¿”å›æœ€ç»ˆç­”æ¡ˆï¼Œä¸è¿”å›æ‰§è¡Œæ­¥éª¤ï¼‰"""
        return {
            "success": success,
            "answer": answer,
            "processing_time": processing_time,
            "error": error
        }

