"""
å¢å¼ºçš„ PDF ç¼–ç å™¨

åŠŸèƒ½ï¼š
1. PDF â†’ å›¾ç‰‡å¸§ â†’ è§†é¢‘ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
2. ç”Ÿæˆæ¯é¡µçš„ VLM Summary
3. å­˜å‚¨åˆ° Doris 4.0ï¼ˆå¯é€‰ï¼‰
"""

import json
import logging
import time
from pathlib import Path
from typing import Optional, List, Dict
import hashlib

from typing import Any
from .pdf_encoder import VisualMemvidEncoder
from .ocr_client import DeepSeekOCRClient
from .gemini_ocr_client import GeminiOCRClient
from .qwen_ocr_client import QwenOCRClient
from .grok_ocr_client import GrokOCRClient
# from .doris_client import DorisClient  # Optional Doris integration
from .config import CONFIG

logger = logging.getLogger(__name__)


class EnhancedPDFEncoder(VisualMemvidEncoder):
    """
    å¢å¼ºçš„ PDF ç¼–ç å™¨

    åœ¨åŸæœ‰ç¼–ç å™¨åŸºç¡€ä¸Šå¢åŠ ï¼š
    1. VLM Summary ç”Ÿæˆ
    2. Doris å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
    """

    def __init__(
        self,
        summary_client: Optional[Any] = None,  # Summaryç”Ÿæˆå®¢æˆ·ç«¯ï¼ˆGemini/DeepSeekç­‰ï¼‰
        ocr_client: Optional[DeepSeekOCRClient] = None,  # å…¨é¡µOCRå®¢æˆ·ç«¯
        doris_client: Optional[Any] = None,  # DorisClient type (optional integration)
        enable_summary: bool = True,
        enable_doris: bool = False
    ):
        """
        åˆå§‹åŒ–å¢å¼ºç¼–ç å™¨

        Args:
            summary_client: Summaryç”Ÿæˆå®¢æˆ·ç«¯ï¼ˆæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ï¼‰
            ocr_client: å…¨é¡µOCRå®¢æˆ·ç«¯ï¼ˆDeepSeek OCRï¼‰
            doris_client: Doris å®¢æˆ·ç«¯
            enable_summary: æ˜¯å¦ç”Ÿæˆ Summary
            enable_doris: æ˜¯å¦å­˜å‚¨åˆ° Doris
        """
        super().__init__()

        # åˆå§‹åŒ– Summary ç”Ÿæˆå®¢æˆ·ç«¯
        if summary_client:
            self.summary_client = summary_client
        else:
            # æ ¹æ®é…ç½®é€‰æ‹© Summary å®¢æˆ·ç«¯
            summary_provider = CONFIG["summary"]["provider"]
            if summary_provider == "gemini":
                self.summary_client = GeminiOCRClient(
                    api_key=CONFIG["api_keys"]["openrouter"],
                    model=CONFIG["summary"]["model"]
                )
                logger.info("âœ… ä½¿ç”¨ Gemini ç”Ÿæˆ Summary")
            elif summary_provider == "qwen":
                self.summary_client = QwenOCRClient(
                    api_key=CONFIG["api_keys"]["openrouter"],
                    model=CONFIG["summary"]["model"]
                )
                logger.info("âœ… ä½¿ç”¨ Qwen ç”Ÿæˆ Summary")
            elif summary_provider == "grok":
                self.summary_client = GrokOCRClient(
                    api_key=CONFIG["api_keys"]["openrouter"],
                    model=CONFIG["summary"]["model"]
                )
                logger.info("âœ… ä½¿ç”¨ Grok-4-Fast ç”Ÿæˆ Summary")
            else:
                # é»˜è®¤ä½¿ç”¨ DeepSeek OCR
                self.summary_client = DeepSeekOCRClient(
                    endpoint=CONFIG["ocr"]["endpoint"]
                )
                logger.info("âœ… ä½¿ç”¨ DeepSeek OCR ç”Ÿæˆ Summary")

        # åˆå§‹åŒ–å…¨é¡µ OCR å®¢æˆ·ç«¯
        self.ocr_client = ocr_client or DeepSeekOCRClient(
            endpoint=CONFIG["ocr"]["endpoint"]
        )

        self.doris_client = doris_client
        self.enable_summary = enable_summary
        self.enable_doris = enable_doris

        if enable_doris and not doris_client:
            logger.warning("âš ï¸ enable_doris=True ä½†æœªæä¾› doris_clientï¼Œå°†ç¦ç”¨ Doris å­˜å‚¨")
            self.enable_doris = False

        logger.info(f"å¢å¼ºç¼–ç å™¨åˆå§‹åŒ–: Summary={enable_summary}, Doris={enable_doris}")
    
    def encode_with_summary(
        self,
        pdf_path: str,
        output_dir: str = "output",
        doc_id: Optional[str] = None
    ) -> Dict:
        """
        ç¼–ç  PDF å¹¶ç”Ÿæˆ Summary
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            doc_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ–‡ä»¶åçš„ MD5ï¼‰
        
        Returns:
            ç¼–ç ç»“æœï¼ŒåŒ…å«è§†é¢‘è·¯å¾„ã€ç´¢å¼•è·¯å¾„ã€Summary ç­‰
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        
        # ç”Ÿæˆæ–‡æ¡£ ID
        if doc_id is None:
            doc_id = hashlib.md5(pdf_path.name.encode()).hexdigest()[:16]
        
        logger.info(f"ğŸ“„ å¼€å§‹ç¼–ç : {pdf_path.name} (doc_id={doc_id})")
        logger.info(f"   PDF è·¯å¾„: {pdf_path}")
        logger.info(f"   è¾“å‡ºç›®å½•: {output_dir}")

        # Phase 1: åŸæœ‰ç¼–ç æµç¨‹ï¼ˆPDF â†’ è§†é¢‘ï¼‰
        logger.info("=" * 60)
        logger.info("Phase 1: PDF â†’ è§†é¢‘ç¼–ç ")
        logger.info("=" * 60)
        start_time = time.time()

        logger.info(f"ğŸ”§ æ·»åŠ  PDF åˆ°ç¼–ç å™¨...")
        self.add_pdf(str(pdf_path))
        print(f"\nâœ… add_pdf() è¿”å›æˆåŠŸï¼total_pages={self.total_pages}", flush=True)
        logger.info(f"âœ… PDF æ·»åŠ æˆåŠŸï¼Œæ€»é¡µæ•°: {self.total_pages}")

        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•...", flush=True)
        output_dir_path = Path(output_dir)
        videos_dir = output_dir_path / "videos"
        indexes_dir = output_dir_path / "indexes"
        videos_dir.mkdir(parents=True, exist_ok=True)
        indexes_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_dir_path}")

        # ç”Ÿæˆè§†é¢‘å’Œç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰doc_idå‘½åï¼‰
        print(f"ğŸ¬ ç”Ÿæˆè§†é¢‘æ–‡ä»¶è·¯å¾„...", flush=True)
        video_path = videos_dir / f"{doc_id}.mp4"
        # BM25S ç´¢å¼•éœ€è¦ä¿å­˜åˆ°ç›®å½•ï¼Œè€Œä¸æ˜¯å•ä¸ª JSON æ–‡ä»¶
        index_path = indexes_dir / f"{doc_id}_index"
        logger.info(f"ğŸ¬ è§†é¢‘è¾“å‡ºè·¯å¾„: {video_path}")
        logger.info(f"ğŸ“‹ ç´¢å¼•è¾“å‡ºè·¯å¾„: {index_path}")

        print(f"ğŸ¥ å‡†å¤‡è°ƒç”¨ build_video()...", flush=True)
        logger.info(f"ğŸ¥ å¼€å§‹æ„å»ºè§†é¢‘...")
        result = self.build_video(str(video_path), str(index_path))
        print(f"âœ… build_video() è¿”å›æˆåŠŸï¼", flush=True)

        # ä¿æŒæˆ‘ä»¬è®¾ç½®çš„æ­£ç¡®è·¯å¾„ï¼Œä¸ä½¿ç”¨ build_video è¿”å›çš„è·¯å¾„
        # video_path å’Œ index_path å·²ç»åœ¨å‰é¢è®¾ç½®å¥½äº†
        print(f"ğŸ“Š ä½¿ç”¨é¢„è®¾è·¯å¾„: video={video_path}, index={index_path}", flush=True)

        print(f"â±ï¸ è®¡ç®—ç¼–ç æ—¶é—´...", flush=True)
        encode_time = time.time() - start_time
        print(f"â±ï¸ ç¼–ç æ—¶é—´: {encode_time:.1f} ç§’", flush=True)

        print(f"ğŸ“ å‡†å¤‡è®°å½•æ—¥å¿— 1...", flush=True)
        logger.info(f"âœ… è§†é¢‘ç¼–ç å®Œæˆ: {encode_time:.1f} ç§’")
        print(f"ğŸ“ æ—¥å¿— 1 å®Œæˆ", flush=True)

        logger.info(f"   è§†é¢‘æ–‡ä»¶: {video_path}")
        print(f"ğŸ“ æ—¥å¿— 2 å®Œæˆ", flush=True)

        logger.info(f"   ç´¢å¼•æ–‡ä»¶: {index_path}")
        print(f"ğŸ“ æ—¥å¿— 3 å®Œæˆ", flush=True)

        # Phase 2: ç”Ÿæˆ Summaryï¼ˆå¦‚æœå¯ç”¨ï¼‰
        print(f"ğŸ”„ è¿›å…¥ Phase 2...", flush=True)
        summaries = []
        if self.enable_summary:
            print(f"âœ… Summary å·²å¯ç”¨", flush=True)
            logger.info("=" * 60)
            logger.info("Phase 2: ç”Ÿæˆ VLM Summary")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š æ€»é¡µæ•°: {self.total_pages}")
            logger.info(f"ğŸ”§ OCR å®¢æˆ·ç«¯å¯ç”¨: {self.ocr_client.is_available if self.ocr_client else False}")

            start_time = time.time()

            summaries = self._generate_summaries(
                doc_id=doc_id,
                doc_name=pdf_path.name
            )

            summary_time = time.time() - start_time
            logger.info(f"âœ… Summary ç”Ÿæˆå®Œæˆ: {summary_time:.1f} ç§’")
            logger.info(f"   æˆåŠŸç”Ÿæˆ: {len(summaries)} ä¸ª Summary")

            # ä¿å­˜ Summary åˆ° JSONï¼ˆæŒ‰æ–‡æ¡£IDåˆ†æ–‡ä»¶å¤¹ï¼‰
            summary_dir = Path(output_dir) / "summaries" / doc_id
            summary_dir.mkdir(parents=True, exist_ok=True)
            summary_path = summary_dir / "summaries.json"
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summaries, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ Summary å·²ä¿å­˜: {summary_path}")
        else:
            logger.info("â­ï¸  è·³è¿‡ Phase 2: Summary ç”Ÿæˆå·²ç¦ç”¨")
        
        # Phase 3: å­˜å‚¨åˆ° Dorisï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_doris and summaries:
            logger.info("Phase 3: å­˜å‚¨åˆ° Doris")
            start_time = time.time()
            
            self._store_to_doris(summaries)
            
            doris_time = time.time() - start_time
            logger.info(f"âœ… Doris å­˜å‚¨å®Œæˆ: {doris_time:.1f} ç§’")
        
        # è¿”å›ç»“æœ
        summary_path = str(Path(output_dir) / "summaries" / doc_id / "summaries.json") if self.enable_summary and summaries else None
        result = {
            "doc_id": doc_id,
            "doc_name": pdf_path.name,
            "video_path": str(video_path),
            "index_path": str(index_path),
            "summary_path": summary_path,
            "total_pages": self.total_pages,
            "summaries": summaries,
            "enable_summary": self.enable_summary,
            "enable_doris": self.enable_doris,
        }
        
        logger.info(f"ğŸ‰ ç¼–ç å®Œæˆ: {pdf_path.name}")
        return result
    
    def _generate_summaries(
        self,
        doc_id: str,
        doc_name: str
    ) -> List[Dict]:
        """
        ä¸ºæ¯ä¸€é¡µç”Ÿæˆ Summary

        Args:
            doc_id: æ–‡æ¡£ID
            doc_name: æ–‡æ¡£åç§°

        Returns:
            Summary åˆ—è¡¨
        """
        print(f"\nğŸ”„ _generate_summaries() å¼€å§‹æ‰§è¡Œ...", flush=True)
        summaries = []
        total_pages = self.total_pages
        print(f"ğŸ“Š æ€»é¡µæ•°: {total_pages}", flush=True)

        logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {total_pages} é¡µçš„ Summary...")

        # æ£€æŸ¥ Summary å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        print(f"ğŸ” æ£€æŸ¥ Summary å®¢æˆ·ç«¯: {self.summary_client}", flush=True)
        if self.summary_client is None:
            print(f"âš ï¸ Summary å®¢æˆ·ç«¯ä¸º None", flush=True)
            logger.warning("âš ï¸ Summary å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ Summary ç”Ÿæˆ")
            return summaries

        print(f"ğŸ” æ£€æŸ¥ Summary å®¢æˆ·ç«¯å¯ç”¨æ€§: {self.summary_client.is_available}", flush=True)
        if not self.summary_client.is_available:
            print(f"âš ï¸ Summary æœåŠ¡ä¸å¯ç”¨", flush=True)
            logger.warning("âš ï¸ Summary æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡ Summary ç”Ÿæˆ")
            return summaries

        print(f"âœ… Summary å®¢æˆ·ç«¯æ£€æŸ¥é€šè¿‡ï¼Œå‡†å¤‡å¤„ç† {total_pages} é¡µ", flush=True)

        # è¿ç»­å¤±è´¥è®¡æ•°å™¨
        consecutive_failures = 0
        max_consecutive_failures = 5  # è¿ç»­å¤±è´¥ 5 æ¬¡ååœæ­¢

        # éå†æ‰€æœ‰é¡µé¢
        print(f"ğŸ” å¼€å§‹éå† {total_pages} é¡µ...", flush=True)
        for page_num in range(1, total_pages + 1):
            frame_num = page_num - 1
            print(f"\nğŸ“„ å¤„ç†ç¬¬ {page_num}/{total_pages} é¡µ...", flush=True)

            logger.info(f"ğŸ“„ å¤„ç†ç¬¬ {page_num}/{total_pages} é¡µ (å¸§ {frame_num})...")

            try:
                # ä»å¸§ç›®å½•è¯»å–å›¾ç‰‡
                frame_path = self.frames_dir / f"page_{frame_num:06d}.png"
                print(f"   ğŸ–¼ï¸  å¸§è·¯å¾„: {frame_path}", flush=True)
                logger.debug(f"   ğŸ–¼ï¸  å¸§è·¯å¾„: {frame_path}")

                if not frame_path.exists():
                    print(f"   âš ï¸ å¸§æ–‡ä»¶ä¸å­˜åœ¨", flush=True)
                    logger.warning(f"   âš ï¸ å¸§æ–‡ä»¶ä¸å­˜åœ¨: {frame_path}")
                    continue

                from PIL import Image
                print(f"   ğŸ“‚ åŠ è½½å›¾ç‰‡...", flush=True)
                frame_img = Image.open(frame_path)
                print(f"   âœ… å›¾ç‰‡åŠ è½½æˆåŠŸ: {frame_img.size}", flush=True)
                logger.debug(f"   âœ… å›¾ç‰‡åŠ è½½æˆåŠŸ: {frame_img.size}")

                # è°ƒç”¨ Summary å®¢æˆ·ç«¯ç”Ÿæˆ Summary
                print(f"   ğŸ”„ å‡†å¤‡è°ƒç”¨ Summary æœåŠ¡...", flush=True)
                logger.info(f"   ğŸ”„ è°ƒç”¨ Summary æœåŠ¡...")
                print(f"   â³ Summary ç”Ÿæˆä¸­ï¼ˆå¯èƒ½éœ€è¦å‡ ç§’ï¼‰...", flush=True)
                result = self.summary_client.ocr_image(
                    frame_img,
                    mode="summary"  # ä½¿ç”¨ summary æ¨¡å¼
                )
                print(f"   âœ… Summary è°ƒç”¨è¿”å›", flush=True)
                print(f"   ğŸ“¦ Summary å®Œæ•´å“åº”: {result}", flush=True)

                logger.debug(f"   ğŸ“¦ Summary å“åº”: success={result.get('success')}, text_length={len(result.get('text', ''))}")

                if result.get("success"):
                    summary_text = result["text"]
                    print(f"   âœ… Summary ç”ŸæˆæˆåŠŸï¼æ–‡æœ¬é•¿åº¦: {len(summary_text)}", flush=True)
                    print(f"   ğŸ“„ Summary åŸå§‹å†…å®¹:\n{'-'*60}\n{summary_text}\n{'-'*60}", flush=True)
                    logger.info(f"   âœ… Summary ç”ŸæˆæˆåŠŸ: æ–‡æœ¬é•¿åº¦ {len(summary_text)}")

                    # é‡ç½®å¤±è´¥è®¡æ•°å™¨
                    consecutive_failures = 0

                    # è§£æ JSONï¼ˆå»é™¤ Markdown ä»£ç å—æ ‡è®°ï¼‰
                    import json

                    # å»é™¤ ```json å’Œ ``` æ ‡è®°
                    json_text = summary_text.strip()
                    if json_text.startswith("```json"):
                        json_text = json_text[7:]  # å»é™¤ ```json
                    elif json_text.startswith("```"):
                        json_text = json_text[3:]  # å»é™¤ ```
                    if json_text.endswith("```"):
                        json_text = json_text[:-3]  # å»é™¤ç»“å°¾çš„ ```
                    json_text = json_text.strip()

                    try:
                        # è§£æ JSON
                        rich_summary = json.loads(json_text)
                        print(f"   âœ… JSON è§£ææˆåŠŸï¼", flush=True)
                        logger.info(f"   âœ… JSON è§£ææˆåŠŸ")

                        # æå–å­—æ®µï¼ˆæ–°ç»“æ„ï¼šåˆ é™¤ summary å’Œ key_wordsï¼‰
                        page_type = rich_summary.get("page_type", "æœªçŸ¥")
                        page_summary = rich_summary.get("page_summary", "")  # ä½¿ç”¨ page_summary è€Œä¸æ˜¯ summary
                        entities = rich_summary.get("entities", [])
                        key_data = rich_summary.get("key_data", [])
                        table_info = rich_summary.get("table_info")
                        chart_info = rich_summary.get("chart_info")
                        image_info = rich_summary.get("image_info")

                        print(f"   ğŸ“„ è§£æåçš„ Summary:\n{'-'*60}\n{page_summary}\n{'-'*60}", flush=True)
                        logger.info(f"   ğŸ“„ page_summary é•¿åº¦: {len(page_summary)}")

                    except json.JSONDecodeError as e:
                        # JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬
                        logger.warning(f"   âš ï¸ JSON è§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
                        print(f"   âš ï¸ JSON è§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬", flush=True)
                        page_type = "æœªçŸ¥"
                        page_summary = summary_text
                        entities = []
                        key_data = []
                        table_info = None
                        chart_info = None
                        image_info = None

                    # æ£€æµ‹ç‰¹æ®Šå†…å®¹
                    has_table = table_info is not None
                    has_formula = "å…¬å¼" in page_summary or "formula" in page_summary.lower()
                    has_chart = chart_info is not None
                    logger.debug(f"   ğŸ“Š å†…å®¹æ£€æµ‹: è¡¨æ ¼={has_table}, å…¬å¼={has_formula}, å›¾è¡¨={has_chart}")

                    # ä¿å­˜ç®€åŒ–çš„ Rich Summaryï¼ˆåˆ é™¤ summary, key_words, keywords, has_* å­—æ®µï¼‰
                    summary = {
                        "doc_id": doc_id,
                        "doc_name": doc_name,
                        "page_num": page_num,
                        "frame_num": frame_num,
                        "page_type": page_type,
                        "page_summary": page_summary,
                        "entities": entities,
                        "key_data": key_data,
                        "table_info": table_info,
                        "chart_info": chart_info,
                        "image_info": image_info,
                        "processing_time": result.get("processing_time", 0)
                    }

                    summaries.append(summary)
                    logger.info(f"   âœ… Summary å·²ä¿å­˜: {page_summary[:80]}...")
                else:
                    consecutive_failures += 1
                    error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                    logger.warning(f"   âš ï¸ Summary ç”Ÿæˆå¤±è´¥ ({consecutive_failures}/{max_consecutive_failures}): {error_msg}")

                    # æ£€æŸ¥æ˜¯å¦è¿ç»­å¤±è´¥è¿‡å¤š
                    if consecutive_failures >= max_consecutive_failures:
                        logger.error(f"âŒ OCR æœåŠ¡è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œåœæ­¢ Summary ç”Ÿæˆ")
                        break

            except Exception as e:
                consecutive_failures += 1
                logger.error(f"    âŒ å¤„ç†ç¬¬ {page_num} é¡µæ—¶å‡ºé”™ ({consecutive_failures}/{max_consecutive_failures}): {e}")

                # æ£€æŸ¥æ˜¯å¦è¿ç»­å¤±è´¥è¿‡å¤š
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"âŒ è¿ç»­å¼‚å¸¸ {consecutive_failures} æ¬¡ï¼Œåœæ­¢ Summary ç”Ÿæˆ")
                    break
        
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(summaries)}/{total_pages} é¡µçš„ Summary")
        return summaries
    

    def _store_to_doris(self, summaries: List[Dict]):
        """
        å­˜å‚¨ Summary åˆ° Doris
        
        Args:
            summaries: Summary åˆ—è¡¨
        """
        if not self.doris_client:
            logger.warning("âš ï¸ Doris å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å­˜å‚¨")
            return
        
        try:
            # æ‰¹é‡æ’å…¥
            self.doris_client.batch_insert_summaries(summaries)
            logger.info(f"âœ… å·²å­˜å‚¨ {len(summaries)} æ¡ Summary åˆ° Doris")
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨åˆ° Doris å¤±è´¥: {e}")
            raise

