"""
æµ‹è¯• Summary ç”Ÿæˆï¼ˆä¸éœ€è¦ Dorisï¼‰

è¿™ä¸ªè„šæœ¬åªæµ‹è¯•ï¼š
1. PDF â†’ è§†é¢‘ç¼–ç 
2. VLM Summary ç”Ÿæˆ
3. Summary ä¿å­˜åˆ° JSON

ä¸éœ€è¦ Dorisï¼Œå¯ä»¥å¿«é€ŸéªŒè¯ Summary åŠŸèƒ½
"""

import sys
import logging
import time
import json
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from visual_memvid import (
    EnhancedPDFEncoder,
    DeepSeekOCRClient,
    CONFIG
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯• Summary ç”Ÿæˆ")
    logger.info("=" * 80)
    
    # æ£€æŸ¥ PDF æ–‡ä»¶
    pdf_path = "2023ä¸­å›½ç¯ä¿å…¬ç›Šç»„ç»‡ç°çŠ¶è°ƒç ”æŠ¥å‘Š.pdf"
    if not Path(pdf_path).exists():
        logger.error(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        logger.info("è¯·å°†æµ‹è¯• PDF æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•")
        return
    
    try:
        # åˆ›å»º OCR å®¢æˆ·ç«¯
        logger.info("åˆå§‹åŒ– DeepSeek OCR å®¢æˆ·ç«¯...")
        ocr_client = DeepSeekOCRClient(endpoint=CONFIG["ocr"]["endpoint"])
        
        # æµ‹è¯•è¿æ¥
        health = ocr_client._check_health()
        if not health:
            logger.error("âŒ DeepSeek OCR æœåŠ¡ä¸å¯ç”¨")
            logger.info(f"è¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨: {CONFIG['ocr']['endpoint']}")
            return
        
        logger.info("âœ… DeepSeek OCR æœåŠ¡æ­£å¸¸")
        
        # åˆ›å»ºå¢å¼ºç¼–ç å™¨ï¼ˆä¸ä½¿ç”¨ Dorisï¼‰
        logger.info("\nåˆ›å»ºå¢å¼ºç¼–ç å™¨...")
        encoder = EnhancedPDFEncoder(
            ocr_client=ocr_client,
            doris_client=None,  # ä¸ä½¿ç”¨ Doris
            enable_summary=True,
            enable_doris=False
        )
        
        # ç¼–ç  PDF
        logger.info(f"\nå¼€å§‹ç¼–ç : {pdf_path}")
        logger.info("=" * 80)
        start_time = time.time()
        
        result = encoder.encode_with_summary(
            pdf_path=pdf_path,
            output_dir="output"
        )
        
        total_time = time.time() - start_time
        
        # æ‰“å°ç»“æœ
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š ç¼–ç ç»“æœ:")
        logger.info("=" * 80)
        logger.info(f"  æ–‡æ¡£ID: {result['doc_id']}")
        logger.info(f"  æ–‡æ¡£å: {result['doc_name']}")
        logger.info(f"  æ€»é¡µæ•°: {result['total_pages']}")
        logger.info(f"  è§†é¢‘è·¯å¾„: {result['video_path']}")
        logger.info(f"  ç´¢å¼•è·¯å¾„: {result['index_path']}")
        logger.info(f"  Summary æ•°é‡: {len(result['summaries'])}")
        logger.info(f"  æ€»è€—æ—¶: {total_time:.1f} ç§’")
        logger.info(f"  å¹³å‡æ¯é¡µ: {total_time/result['total_pages']:.1f} ç§’")
        logger.info("=" * 80)
        
        # æ‰“å°å‰ 5 é¡µçš„ Summary
        logger.info("\nğŸ“„ å‰ 5 é¡µçš„ Summary:")
        logger.info("=" * 80)
        for i, summary in enumerate(result['summaries'][:5]):
            logger.info(f"\nç¬¬ {summary['page_num']} é¡µ:")
            logger.info(f"  Summary: {summary['summary']}")
            logger.info(f"  å…³é”®è¯: {', '.join(summary['keywords'][:10])}")
            logger.info(f"  ç‰¹å¾: è¡¨æ ¼={summary['has_table']}, å…¬å¼={summary['has_formula']}, å›¾è¡¨={summary['has_chart']}")
            logger.info(f"  å¤„ç†æ—¶é—´: {summary['processing_time']:.1f}ç§’")
        
        # ä¿å­˜å®Œæ•´ Summary åˆ°æ–‡ä»¶
        summary_file = Path("output") / "summaries.json"
        logger.info(f"\nğŸ’¾ å®Œæ•´ Summary å·²ä¿å­˜åˆ°: {summary_file}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        logger.info("=" * 80)
        total_keywords = sum(len(s['keywords']) for s in result['summaries'])
        table_pages = sum(1 for s in result['summaries'] if s['has_table'])
        formula_pages = sum(1 for s in result['summaries'] if s['has_formula'])
        chart_pages = sum(1 for s in result['summaries'] if s['has_chart'])
        
        logger.info(f"  æ€»å…³é”®è¯æ•°: {total_keywords}")
        logger.info(f"  å¹³å‡æ¯é¡µå…³é”®è¯: {total_keywords/len(result['summaries']):.1f}")
        logger.info(f"  åŒ…å«è¡¨æ ¼çš„é¡µæ•°: {table_pages}")
        logger.info(f"  åŒ…å«å…¬å¼çš„é¡µæ•°: {formula_pages}")
        logger.info(f"  åŒ…å«å›¾è¡¨çš„é¡µæ•°: {chart_pages}")
        
        # æµ‹è¯• Summary æœç´¢ï¼ˆç®€å•å…³é”®è¯åŒ¹é…ï¼‰
        logger.info("\nğŸ” æµ‹è¯• Summary æœç´¢:")
        logger.info("=" * 80)
        
        test_queries = [
            "ç¯ä¿å…¬ç›Šç»„ç»‡",
            "614",
            "é—®å·è°ƒç ”",
            "å æ¯”"
        ]
        
        for query in test_queries:
            logger.info(f"\næŸ¥è¯¢: {query}")
            matches = []
            for summary in result['summaries']:
                if query in summary['summary'] or query in ','.join(summary['keywords']):
                    matches.append(summary)
            
            logger.info(f"  æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…é¡µé¢")
            for match in matches[:2]:
                logger.info(f"    ç¬¬ {match['page_num']} é¡µ: {match['summary'][:60]}...")
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        logger.info("=" * 80)
        
        logger.info("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        logger.info("  1. æŸ¥çœ‹ output/summaries.json äº†è§£å®Œæ•´ Summary")
        logger.info("  2. å¦‚æœæœ‰ Dorisï¼Œè¿è¡Œ test_doris_integration.py æµ‹è¯•å®Œæ•´åŠŸèƒ½")
        logger.info("  3. ä½¿ç”¨ test_retrieve.py æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

